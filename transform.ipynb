{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08a8e3d2-8161-495d-8674-457278619e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "from google.cloud import storage\n",
    "import google.cloud.storage\n",
    "\n",
    "import _pickle as cPickle\n",
    "#import gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca593a94-c491-454f-8044-c3bc8bded7c2",
   "metadata": {},
   "source": [
    "Initialize the paths for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e85fe77a-c21c-4eb2-9e53-9835b595babd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_word = 50\n",
    "stop_word = 1e4\n",
    "unknown = 1\n",
    "\n",
    "input_file = './data.txt'\n",
    "vocab_file = './vocab.txt'\n",
    "stop_file = './stop.txt'\n",
    "vocab_pkl = './vocab.pkl'\n",
    "#input_file = './data.txt'\n",
    "#vocab_file = './vocab.txt'\n",
    "#stop_file = './stop.txt'\n",
    "#vocab_pkl = './vocab.pkl'\n",
    "\n",
    "bucketname = \"sdata-team60-paper168\"\n",
    "\n",
    "fs = None#gcsfs.GCSFileSystem(project='paper-168-team60')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f9ad6f23-de16-4001-882f-324da22ae207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(path, obj):\n",
    "    #fs = gcsfs.GCSFileSystem(project='paper-168-team60')\n",
    "    with open(path, 'wb') as f:\n",
    "        cPickle.dump(obj, f)\n",
    "        print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_pkl(path):\n",
    "    with open(path) as f:\n",
    "        obj = cPickle.load(f)\n",
    "        print(\" [*] load %s\" % path)\n",
    "    return obj\n",
    "\n",
    "def save_npy(path, obj):\n",
    "    np.save(path, obj)\n",
    "    print(\" [*] save %s\" % path)\n",
    "\n",
    "def load_npy(path):\n",
    "    obj = np.load(path)\n",
    "    print(\" [*] load %s\" % path)\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e45267c-e0b6-43d8-a7c4-6bcdb41dc89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations for the model, retaining same configurations as original code.\n",
    "class Config:\n",
    "    \"\"\"feel free to play with these hyperparameters during training\"\"\"\n",
    "    dataset = \"resource\"  # change this to the right data name\n",
    "    data_path = \"../%s\" % dataset\n",
    "    checkpoint_dir = \"checkpoint\"\n",
    "    decay_rate = 0.95\n",
    "    decay_step = 1000\n",
    "    n_topics = 50\n",
    "    learning_rate = 0.00002\n",
    "    vocab_size = 619\n",
    "    n_stops = 22 \n",
    "    lda_vocab_size = vocab_size - n_stops\n",
    "    n_hidden = 200\n",
    "    n_layers = 2\n",
    "    dropout = 1.0\n",
    "    max_grad_norm = 1.0 #for gradient clipping\n",
    "    total_epoch = 5\n",
    "    init_scale = 0.075\n",
    "    threshold = 0.5 #probability cut-off for predicting label to be 1\n",
    "    forward_only = False #indicates whether we are in testing or training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8561db2d-5f29-40c0-92cb-d6b2b409d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the data and formating for futher preprocessing\n",
    "def dump_vocab():\n",
    "    df = pd.read_csv(\"./data.txt\" ,dtype={\"PID\": int, \"DAY_ID\": int,\"DX_GROUP_DESCRIPTION\":\"string\",\"SERVICE_LOCATION\": \"string\",\"OP_DATE\":int }, sep=\"\\t\", header=0)\n",
    "    #print(df[0:3])\n",
    "    hist = df.groupby('DX_GROUP_DESCRIPTION').size().to_frame('SIZE').reset_index()\n",
    "    #print(hist[0:3])\n",
    "    #print(\"************************\")\n",
    "    hist_sort = hist.sort_values(by='SIZE', ascending=False)\n",
    "    #print(hist_sort[0:3])\n",
    "    #print(\"****************************************************************\")\n",
    "    count = hist.groupby('SIZE').size().to_frame('COUNT').reset_index()\n",
    "    #print(count)\n",
    "\n",
    "    ##Removing low frequency words\n",
    "\n",
    "    hist = hist[hist['SIZE'] > rare_word]\n",
    "    #print(hist)\n",
    "    ##Remaining entries make the vocab anf formatting\n",
    "    vocab = hist.sort_values(by='SIZE').reset_index()['DX_GROUP_DESCRIPTION']\n",
    "    print(vocab)\n",
    "    vocab.index += 2  # reserve 1 to unk\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucketname)\n",
    "    bucket.blob(\"vocab.txt\").upload_from_string(vocab.to_csv( sep='\\t', header=False, index=True),'text')\n",
    "\n",
    "\n",
    "    hist[hist['SIZE'] > stop_word].reset_index()['DX_GROUP_DESCRIPTION']\\\n",
    "            .to_csv(stop_file, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2106612-c97f-46a3-8605-bc1b96bb5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(fs):\n",
    "    #Building the index keys for words\n",
    "\n",
    "    word_to_index = {}\n",
    "\n",
    "    with open(vocab_file, 'r') as f:\n",
    "        line = f.readline()\n",
    "        while line != '':\n",
    "            #print(line)\n",
    "            tokens = line.strip().split('\\t')\n",
    "            #print(\"tokens looks like\")\n",
    "            #print(tokens)\n",
    "            #print(\"tokens[1] : \", tokens[1] )\n",
    "            #print(\"tokens[0] : \", tokens[0])\n",
    "            word_to_index[tokens[1]] = int(tokens[0])\n",
    "            line = f.readline()\n",
    "    save_pkl(vocab_pkl, {v: k for k, v in word_to_index.items()})\n",
    "    print('dict size: ' + str(len(word_to_index)))\n",
    "    return word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "095ecbd9-6318-4145-8d70-4d37824498a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format(word_to_index, events):\n",
    "    # order by PID, DAY_ID\n",
    "    with open(input_file, mode='r') as f:\n",
    "        # header\n",
    "        header = f.readline().strip().split('\\t')\n",
    "        print(header)\n",
    "        pos = {}\n",
    "        for key, value in enumerate(header):\n",
    "            pos[value] = key\n",
    "        print(pos)\n",
    "\n",
    "        docs = []\n",
    "        doc = []\n",
    "        sent = []\n",
    "        labels = []\n",
    "        label = []\n",
    "\n",
    "        # init\n",
    "        line = f.readline()\n",
    "        tokens = line.strip().split('\\t')\n",
    "        pid = tokens[pos['PID']]\n",
    "        day_id = tokens[pos['DAY_ID']]\n",
    "        label.append(tag(events, pid, day_id))\n",
    "\n",
    "        while line != '':\n",
    "            tokens = line.strip().split('\\t')\n",
    "            c_pid = tokens[pos['PID']]\n",
    "            c_day_id = tokens[pos['DAY_ID']]\n",
    "\n",
    "            # closure\n",
    "            if c_pid != pid:\n",
    "                doc.append(sent)\n",
    "                docs.append(doc)\n",
    "                labels.append(label)\n",
    "                sent = []\n",
    "                doc = []\n",
    "                label = []\n",
    "                pid = c_pid\n",
    "                day_id = c_day_id\n",
    "                label.append(tag(events, pid, day_id))\n",
    "            else:\n",
    "                if c_day_id != day_id:\n",
    "                    doc.append(sent)\n",
    "                    sent = []\n",
    "                    day_id = c_day_id\n",
    "                    label.append(tag(events, pid, day_id))\n",
    "\n",
    "            word = tokens[pos['DX_GROUP_DESCRIPTION']]\n",
    "            try:\n",
    "                sent.append(word_to_index[word])\n",
    "            except KeyError:\n",
    "                sent.append(unknown)\n",
    "\n",
    "            line = f.readline()\n",
    "\n",
    "        # closure\n",
    "        doc.append(sent)\n",
    "        docs.append(doc)\n",
    "        labels.append(label)\n",
    "        for i in range(len(labels[0])):\n",
    "            print(doc[0][i] , \"- \" ,labels[0][i])\n",
    "       \n",
    "\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "82746659-ff9c-419c-84c3-62be9292c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(docs, labels):\n",
    "    # train, validate, test\n",
    "    # X, Y,\n",
    "    # TODO: YY\n",
    "    print(len(docs))\n",
    "    #print(docs)\n",
    "    print(len(labels))\n",
    "    #print(labels)\n",
    "\n",
    "    save_pkl('./X_train.pkl', docs[:2000])\n",
    "    save_pkl('./Y_train.pkl', labels[:2000])\n",
    "    save_pkl('./X_valid.pkl', docs[2000:2700])\n",
    "    save_pkl('./Y_valid.pkl', labels[2000:2700])\n",
    "    save_pkl('./X_test.pkl', docs[2700:])\n",
    "    save_pkl('./Y_test.pkl', labels[2700:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "380eb340-b766-4903-9916-b4bb20cc4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events():\n",
    "    # extract event \"INPATIENT HOSPITAL\"\n",
    "    target_event = 'INPATIENT HOSPITAL'\n",
    "\n",
    "    df = pd.read_csv(input_file, sep='\\t', header=0)\n",
    "    events = df[df['SERVICE_LOCATION'] == target_event]\n",
    "\n",
    "    events = events.groupby(['PID', 'DAY_ID', 'SERVICE_LOCATION']).size().to_frame('COUNT').reset_index()\\\n",
    "        .sort_values(by=['PID', 'DAY_ID'], ascending=True)\\\n",
    "        .set_index('PID')\n",
    "\n",
    "    return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "92c333a9-db24-431a-95b9-a73e04b671ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(events, pid, day_id):\n",
    "    return 1 if tag_logic(events, pid, day_id) else 0\n",
    "\n",
    "\n",
    "def tag_logic(events, pid, day_id):\n",
    "    try:\n",
    "        patient = events.loc[int(pid)]\n",
    "\n",
    "        # test whether have events within 30 days\n",
    "        if isinstance(patient, pd.Series):\n",
    "            return (int(day_id) <= patient.DAY_ID) & (patient.DAY_ID < int(day_id) + 30)\n",
    "\n",
    "        return patient.loc[(int(day_id) <= patient.DAY_ID) & (patient.DAY_ID < int(day_id) + 30)].shape[0] > 0\n",
    "    except KeyError:\n",
    "        # the label is not in the [index]\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "631641bb-733f-411a-8c45-78915c5de956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] save ./vocab.pkl\n",
      "dict size: 490\n",
      "['PID', 'DAY_ID', 'DX_GROUP_DESCRIPTION', 'SERVICE_LOCATION', 'OP_DATE']\n",
      "{'PID': 0, 'DAY_ID': 1, 'DX_GROUP_DESCRIPTION': 2, 'SERVICE_LOCATION': 3, 'OP_DATE': 4}\n",
      "194 -  0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k6/x52hhfys3bzf1rmrzq9wqznh0000gn/T/ipykernel_25239/2624553163.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# verify loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/k6/x52hhfys3bzf1rmrzq9wqznh0000gn/T/ipykernel_25239/2624553163.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/k6/x52hhfys3bzf1rmrzq9wqznh0000gn/T/ipykernel_25239/3401380111.py\u001b[0m in \u001b[0;36mconvert_format\u001b[0;34m(word_to_index, events)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"- \"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # dump_vocab()\n",
    "    word_to_index = load_vocab(fs)\n",
    "    events = extract_events()\n",
    "\n",
    "    docs, labels = convert_format(word_to_index, events)\n",
    "    split_data(docs, labels)\n",
    "\n",
    "    # verify loading\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379eec0-af8e-488b-be9f-5c07d98e0a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbdd21-8e2c-4757-8448-3b5870f8b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d8893-14de-4892-b804-4eb1a09f7a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
